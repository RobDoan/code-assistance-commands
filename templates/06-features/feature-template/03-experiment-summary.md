# Experiment Summary: *[Feature Name]*

**Feature ID:** *FEAT-XXXX*  
**Experiment Duration:** *[Start Date]* - *[End Date]*  
**Team:** *[Team members involved]*  
**Completed:** *[Date]*  

---

## Original Hypothesis
>
> **What we believed at the start**

**Core Assumption:** *[Copy from requirements.md - e.g., "We believe that users will adopt the new dashboard because it reduces report generation time"]*

**Success Metrics Defined:**

- **Primary:** *[e.g., 20% increase in weekly active users for reporting]*
- **Secondary:** *[e.g., 30% reduction in average time to generate reports]*
- **Guardrail:** *[e.g., No increase in error rates >5%]*

**Failure Criteria:** *[e.g., Less than 10% user adoption after 2 weeks]*

---

## Experiment Results
>
> **What actually happened - data over drama**

### Quantitative Results

| Metric | Target | Actual Result | Status |
|--------|--------|---------------|--------|
| **Primary Metric** | *[Target value]* | *[Actual value]* | ✅ Hit / ❌ Miss / 🔄 Inconclusive |
| **Secondary Metric** | *[Target value]* | *[Actual value]* | ✅ Hit / ❌ Miss / 🔄 Inconclusive |
| **Guardrail Metric** | *[Target value]* | *[Actual value]* | ✅ Hit / ❌ Miss / 🔄 Inconclusive |

### Qualitative Insights

- **User Feedback:** *[Key themes from user interviews, support tickets, or surveys]*
- **Behavioral Observations:** *[What users actually did vs. what we expected]*
- **Unexpected Findings:** *[Surprising discoveries that weren't in our original hypothesis]*

### Statistical Significance

- **Sample Size:** *[Number of users in experiment]*
- **Confidence Level:** *[e.g., 95%]*
- **P-value:** *[Statistical significance of results]*
- **Effect Size:** *[Magnitude of the difference observed]*

---

## Decision & Outcome
>
> **What we're doing with these learnings**

**Final Decision:**

- [ ] **✅ Persevere** - Hypothesis validated, continue building
- [ ] **🔄 Pivot** - Partial validation, iterate on approach  
- [ ] **💀 Kill** - Hypothesis invalidated, stop development

**Reasoning:** *[2-3 sentences explaining why this decision makes sense given the data]*

---

## 🎉 Celebration of Learning
>
> **Making failure valuable - quantifying what we saved**

### If Hypothesis Was Invalidated (💀 Kill)

**Time Saved:** *[e.g., 6 months of development time]*  
**Money Saved:** *[e.g., $150,000 in development costs]*  
**Opportunity Cost Avoided:** *[e.g., Could have delayed 2 other high-impact features]*  
**Team Impact:** *[e.g., 3 engineers can now work on validated opportunities]*

**🏆 Why This Is A Win:**
*[2-3 sentences celebrating the early learning and avoided waste. This invalidated assumption saved us from building something users didn't want.]*

### If Hypothesis Was Validated (✅ Persevere)

**Confidence Gained:** *[What we now know for certain about user behavior]*  
**Risk Reduced:** *[What major risks have been eliminated]*  
**Investment Justified:** *[ROI or business case validation]*  

---

## Key Learnings
>
> **What we discovered about our users, market, or product**

### About Our Users

1. **User Behavior:** *[e.g., "Users actually care more about data accuracy than speed"]*
2. **Pain Points:** *[e.g., "The biggest friction was in the onboarding flow, not the core feature"]*
3. **Adoption Patterns:** *[e.g., "Power users adopted immediately, casual users needed more guidance"]*

### About Our Product

1. **Technical Insights:** *[e.g., "Performance bottleneck was in the database queries, not the frontend"]*
2. **UX Insights:** *[e.g., "Users expected mobile-first experience even for complex features"]*
3. **Integration Insights:** *[e.g., "Existing workflow integration was more important than new features"]*

### About Our Process

1. **What Worked:** *[e.g., "Feature flags allowed us to iterate quickly without full deployments"]*
2. **What Didn't:** *[e.g., "We should have talked to more users before building"]*
3. **What We'll Do Differently:** *[e.g., "Start with user interviews, not technical spikes"]*

---

## Impact on Strategy
>
> **How these learnings change our roadmap**

### Immediate Changes

- **Roadmap Impact:** *[What features are now higher/lower priority]*
- **Resource Allocation:** *[How team focus is shifting]*
- **Metrics Evolution:** *[What we'll measure differently going forward]*

### Longer-term Implications

- **Product Strategy:** *[How this affects our product direction]*
- **Market Understanding:** *[What we learned about our target market]*
- **Competitive Position:** *[How this affects our competitive advantage]*

---

## Next Experiments
>
> **What hypotheses emerged from this experiment**

### New Hypotheses to Test

1. **Hypothesis:** *[e.g., "Users will engage more with personalized onboarding"]*
   - **Why:** *[What from this experiment led to this hypothesis]*
   - **Proposed Test:** *[How we'd validate this]*

2. **Hypothesis:** *[Next assumption to test]*
   - **Why:** *[Reasoning based on current learnings]*
   - **Proposed Test:** *[Experiment design idea]*

### Follow-up Questions

- *[What questions did this experiment raise that we still need to answer?]*
- *[What user segments should we explore next?]*
- *[What technical approaches should we investigate?]*

---

## Artifacts & References

- **Experiment Design:** [../requirements.md](../requirements.md)
- **Implementation:** [../design.md](../design.md)
- **Raw Data:** *[Link to analytics dashboard, A/B test results]*
- **User Research:** *[Links to interview recordings, survey results]*
- **Technical Postmortem:** *[Link to any technical retrospective]*
- **Stakeholder Presentation:** *[Link to results presentation]*

---

## Team Retrospective
>
> **How we worked together and what we learned about our process**

### What Went Well

- *[Process or collaboration wins]*
- *[Tools or methods that helped]*
- *[Team dynamics that supported learning]*

### What We'd Improve

- *[Process changes for next experiment]*
- *[Communication improvements needed]*
- *[Tools or methods to adopt]*

### Shoutouts & Recognition

- *[Team members who went above and beyond]*
- *[Cross-functional collaboration wins]*
- *[External partners who helped]*

---

## Final Reflection
>
> **The meta-learning: What did we learn about learning?**

*[2-3 paragraphs reflecting on how well we followed our Learning Machine principles, what we'd do differently in the hypothesis formation and testing process, and how this experiment contributes to our overall organizational learning.]*

---

**📋 Template Completed By:** *@username*  
**📅 Date:** *[Date]*  
**🔗 Shared With:** *[Team channels, stakeholders notified]*  
**🎯 Next Experiment:** *[Link to next feature requirements.md]*

---

> *"The goal is not to be right; it's to get better. This experiment made us better."*
